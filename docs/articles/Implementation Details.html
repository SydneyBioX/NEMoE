<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Implementation details • NEMoE</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Implementation details">
<meta property="og:description" content="NEMoE">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">NEMoE</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/NEMoE.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Implementation%20Details.html">Implementation details</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="Implementation%20Details_files/header-attrs-2.10/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Implementation details</h1>
            
      
      
      <div class="hidden name"><code>Implementation Details.Rmd</code></div>

    </div>

    
    
<div id="nemoe-algorithm" class="section level1">
<h1 class="hasAnchor">
<a href="#nemoe-algorithm" class="anchor"></a>NEMoE algorithm</h1>
<p>The implementation of NEMoE involves the EM algorithm, a special case
of the Minorise-Maximisation (MM) algorithm<span class="citation">(Hunter and Lange 2004)</span> and widely used in
estimating parameters for finite mixture models. By alternatively
inferring the latent variables given the parameters (E steps), and then
optimising the parameters given the ``filled in’’ data, the EM algorithm
iteratively finds an appropriate local maximiser of the log-likelihood
function.</p>
<p>In this section, we introduce the implementation of NEMoE. We first
describe the EM algorithm parameters estimation for RMoE, and then
introduce the EM algorithm for NEMoE. To achieve robust parameter
estimation, we use three different strategies for initialisation. We
also implement different variants of the EM algorithm to suit different
scales of the problem. Finally, the selection of the tuning parameter in
NEMoE is also introduced.</p>
<p>For a transformed microbiome data at taxonomic level <span class="math inline">\(l\)</span>, we use the matrix <span class="math inline">\(X_{n \times p_l^{(l)}}\)</span>to denote the
relative abundance of <span class="math inline">\(n\)</span> samples of
<span class="math inline">\(p_l\)</span> taxa. The corresponding diet
information, measured as a nutrients intake matrix, is denoted as <span class="math inline">\(W_{n \times q}\)</span>, where the <span class="math inline">\(q\)</span> columns are the nutrient metrics for
the same <span class="math inline">\(n\)</span> samples. Let <span class="math inline">\(Y_n\)</span> denote the binary response of the
health outcome, with <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span> representing individuals with and
without disease, respectively. NEMoE models the heterogeneous
relationship between the microbiome and the health outcome by a mixture
distribution, i.e.</p>
<p><span class="math display">\[\begin{equation}\label{gating}
    \operatorname{P}_l(Y=1|X^{(l)},W)=\sum_{k=1}^K \pi_k
\frac{\exp(X^{(l)} \beta_k^{(l)})}{1 + \exp(X^{(l)} \beta_k^{(l)})},
\end{equation}\]</span> where <span class="math inline">\(\pi_k=
\frac{\exp(W \gamma_k)}{\sum_{i = 1}^K \exp(W \gamma_i)}\)</span> is the
nutrition class mixing weight of shared components determined by
nutrients intake, and where <span class="math inline">\(\beta_k^{(l)}\)</span> and <span class="math inline">\(\gamma_k\)</span> are the corresponding effect
size for the gating network and the experts network, respectively, and
<span class="math inline">\(K\)</span> denotes the predetermined number
of nutrition classes. NEMoE estimates the regularised sum of all levels
of the log-likelihood (LL) function, where the regularisation term
consists of elastic net penalties for both the gating network and the
experts network: <span class="math display">\[\begin{equation}\label{LL}
    \operatorname{rLL}=\sum_{l=1}^L\sum_{k=1}^K\{\sum_{i=1}^n
\operatorname{log}[P(Y_i|X_i^{(l)}, W_i)] -
\phi(\lambda_{1k}^{(l)},\alpha_{1k}^{(l)}, \beta_k^{(l)} \} -
\phi(\lambda_2, \alpha_2, \gamma),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\phi (\lambda, \alpha,
\beta)=\lambda[\alpha + \frac{1}{2} (1- \alpha) ||\beta||_2^2]\)</span>
is the elastic net penalty function and <span class="math inline">\(\lambda_{1k}^{(l)}\)</span>, <span class="math inline">\(\alpha_{1k}^{(l)}\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\alpha_2\)</span> are the corresponding parameters
for penalties in the experts network and in the gating function.</p>
<div id="em-algorithm-of-rmoe" class="section level2">
<h2 class="hasAnchor">
<a href="#em-algorithm-of-rmoe" class="anchor"></a>EM algorithm of RMoE</h2>
<p>In this section, we first derive the optimisation for RMoE, i.e. set
<span class="math inline">\(L=1\)</span> in rLL. We denote <span class="math inline">\({\theta}=\left\{ {\gamma},{\beta}\right\}\)</span>
and <span class="math inline">\({\theta}^{(t)}=\left\{
{\gamma}^{(t)},{\beta}^{(t)} \right\}\)</span> as the parameters in the
<span class="math inline">\(t^{th}\)</span> iteration. For the
regularised log-likelihood function in Equation rLL, the EM algorithm
runs as follows:</p>
<p> Compute the conditional expectation of the complete data
log-likelihood function given the observed data <span class="math inline">\(D\)</span> and current parameter, the
corresponding expected complete data log-likelihood is as follows:</p>
<p><span class="math display">\[\begin{equation} \label{Q}
\begin{aligned}
    Q({\theta}, {\theta^{(t)}}) &amp; =
\mathbb{E}[PL(\theta)|D;\theta^{(t)}] \\
    &amp; = \sum_{i=1}^n \sum_{k=1} ^K r_{i
k}^{(t)}[\log{\pi_{ik}({w_i},{\gamma})} + p_i(\beta_k;{x_i}, y_i)] \\
    &amp; - \sum_{k=1}^{K} \lambda_{1k} \left[ \alpha_{1k} |\beta_k|+
\frac{1}{2} (1-\alpha_{1k})||\beta_k||_2^2 \right] - \lambda_2\left[
\alpha_2 |{\gamma}| + \frac{1}{2}(1 - \alpha_2) ||\gamma||_2^2 \right],
\end{aligned}
\end{equation}\]</span></p>
<p> where <span class="math inline">\(p_i(\beta_k;{x_i}, y_i) = y_i
\log{\left[{\frac{\exp{({x_i}^T \beta_k)}}{1 + \exp{({x_i}^T
\beta_k)}}}\right]} + (1 - y_i) \log{\left[{\frac{1}{1 + \exp{({x_i}^T
\beta_k)}}}\right]}\)</span> is the log-likelihood function of the
logistic distribution and</p>
<p><span class="math display">\[\begin{equation} \label{prop}
\begin{aligned}
r_{ik}^{(t)} = \frac{\pi_{ik}({w_i},{\gamma}^{(t)})
p_i(\beta_k^{(t)};{x_i}, y_i)}{\sum_{k=1}^K
\pi_{ik}({w_i},{\gamma}^{(t)}) p_i(\beta_k ^{(t)};{x_i}, y_i)}
\end{aligned}
\end{equation}\]</span></p>
<p> is the responsibility that latent class <span class="math inline">\(k\)</span> takes for sample <span class="math inline">\(i\)</span>.</p>
<p> In the M step, we maximise the expected complete data log likelihood
function in Q function. This maximisation can be achieved by maximising
<span class="math inline">\({\beta}\)</span> and <span class="math inline">\({\gamma}\)</span> as follows:</p>
<p><span class="math display">\[\begin{align}
    \beta_k^{(t+1)} &amp; = \operatorname{argmax}_{\beta_k \in
\mathbb{R}^p} \sum_{i=1}^n r_{ik}^{(t)} p_i(\beta_k;{x_i}, y_i) -
\lambda_{1k} \left[ \alpha_{1k} |\beta_k|+ \frac{1}{2}
(1-\alpha_{1k})||\beta_k||_2^2 \right],  \label{beta_up} \\
    {\gamma}^{(t+1)} &amp; =  \operatorname{argmax}_{\gamma \in
\mathbb{R}^q} \sum_{k=1}^K \sum_{i=1}^n r_{ik}^{(t)}\operatorname{log}
\pi_{ik}({w_i},{\gamma}) -  \lambda_2\left[ \alpha_2 |{\gamma}| +
\frac{1}{2}(1 - \alpha_2) ||\gamma||_2^2 \right]. \label{gamma_up}
\end{align}\]</span></p>
<p>From Equation above, it can be seen that the updating of <span class="math inline">\(\beta_k\)</span> is a weighted sparse logistic
regression problem with <span class="math inline">\({x_i}\)</span> as
covariates, <span class="math inline">\(y_i\)</span> as response and
<span class="math inline">\(r_{ik}^{(t)}\)</span> as weight for <span class="math inline">\(i^{th}\)</span> sample. Equation <span class="math inline">\({\gamma}^{(t+1)}\)</span> shows that the updating
of <span class="math inline">\({\gamma}\)</span> is also a sparse
multinomial regression problem with <span class="math inline">\(\pi_{ik}({w_i},{\gamma})\)</span>, defined in
Equation gating networks, as covariates, <span class="math inline">\(r_{ik}^{(t)}\)</span> as response.</p>
<p>To solve the optimisation <span class="math inline">\(\beta_k^{(t+1)}\)</span> and <span class="math inline">\({\gamma}^{(t+1)}\)</span>, a proximal-Newton
iteration approach can be used, which repeatedly approximates the
regularised log-likelihood by a quadratic function.</p>
</div>
<div id="mm-algorithm-of-nemoe" class="section level2">
<h2 class="hasAnchor">
<a href="#mm-algorithm-of-nemoe" class="anchor"></a>MM algorithm of NEMoE</h2>
<p>For the regularised log-likelihood function in Equation rLL, we use
the MM algorithm to estimate the parameters.</p>
<p>Let <span class="math inline">\(\Theta = \left\{ {\beta}^{(1)},
\ldots, {\beta}^{(L)}, {\gamma} \right\}\)</span> be aggregated
parameters in Equation rLL. The estimated parameters and log likelihood
function at <span class="math inline">\(t^{th}\)</span> iteration is
<span class="math inline">\(\Theta^{(t)}\)</span> and <span class="math inline">\({\operatorname{rLL}}_m(\Theta^{(t)})\)</span>. We
have <span class="math display">\[\begin{equation}\label{LL_m}
    \begin{aligned}
        {\operatorname{rLL}}_m(\Theta) -
{\operatorname{rLL}}_m(\Theta^{(t)}) \geq &amp; \sum_{l=1}^L
\sum_{i=1}^n \sum_{k=1} ^K r_{i
k}^{(l),(t)}[\log{\pi_{ik}({w_i},{\gamma})} + p_i(\beta_k^{(l)};{x_i},
y_i)] \\
        &amp; - \sum_{l=1}^L \sum_{i=1}^n \sum_{k=1} ^K r_{i
k}^{(l),(t)}[\log{\pi_{ik}({w_i},{\gamma}^{(t)})} +
p_i(\beta_k^{(l),(t)};{x_i}, y_i)],
    \end{aligned}
\end{equation}\]</span></p>
<p> where <span class="math display">\[\begin{equation} \label{prop_m}
\begin{aligned}
r_{ik}^{(l),(t)} = \frac{\pi_{ik}({w_i},{\gamma}^{(t)})
p_i(\beta_k^{(l),(t)};{x_i}^{(l)}, y_i)}{\sum_{k=1}^K
\pi_{ik}({w_i},{\gamma}^{(t)}) p_i(\beta_k ^{(l),(t)};{x_i}^{(l)},
y_i)}.
\end{aligned}
\end{equation}\]</span></p>
<p>Similar to previous Q function, we can define the multi-level <span class="math inline">\(Q\)</span> function as</p>
<p><span class="math display">\[\begin{equation}\label{Q_m}
    \begin{aligned}
        Q(\Theta,\Theta^{(t)})&amp; = \sum_{l=1}^L \sum_{i=1}^n
\sum_{k=1} ^K r_{i k}^{(l),(t)}[\log{\pi_{ik}({w_i},{\gamma})} +
p_i(\beta_k^{(l)};{x_i}^{(l)}, y_i)] \\
    &amp; - \sum_{l=1}^L\sum_{k=1}^{K} \lambda_{1k}^{(l)} \left[
\alpha_{1k}^{(l)} |\beta_k^{(l)}|+ \frac{1}{2}
(1-\alpha_{1k}^{(l)})||\beta_k^{(l)}||_2^2 \right]  \\
    &amp; - \lambda_2\left[ \alpha_2 |{\gamma}| + \frac{1}{2}(1 -
\alpha_2) ||\gamma||_2^2 \right].
    \end{aligned}
\end{equation}\]</span></p>
<p>With this definition of the <span class="math inline">\(Q\)</span>
function, we have</p>
<p><span class="math display">\[\begin{equation}
    {\operatorname{rLL}}_m(\Theta) -
{\operatorname{rLL}}_m(\Theta^{(t)}) \geq Q(\Theta, \Theta^{(t)}) -
Q(\Theta^{(t)}, \Theta^{(t)}).
\end{equation}\]</span></p>
<p>Thus <span class="math inline">\(Q(\Theta, \Theta^{(t)}) -
Q(\Theta^{(t)}, \Theta^{(t)}) +
\operatorname{rLL}_m(\Theta^{(t)})\)</span> is a minoriser of <span class="math inline">\(\operatorname{rLL}_m(\Theta)\)</span> at <span class="math inline">\(\Theta^{(t)}\)</span>. Similar to the EM
algorithm, a local maximum can be computed by maximising <span class="math inline">\(Q(\Theta,\Theta^{(t)})\)</span> iteratively.
Similar to equations <span class="math inline">\(\beta_k^{(t+1)}\)</span> and <span class="math inline">\({\gamma}^{(t+1)}\)</span>, we have</p>
<p><span class="math display">\[\begin{align}
    \hat{\beta}_k^{(l),(t+1)} &amp; =
\operatorname{argmax}_{\beta_k^{(l)} \in \mathbb{R}^{p_l}} \sum_{i=1}^n
r_{ik}^{(l),(t)} p_i(\beta_k^{(l)};{x_i}^{(l)}, y_i) -
\lambda_{1k}^{(l)} \left[ \alpha_{1k}^{(l)} |\beta_k^{(l)}|+ \frac{1}{2}
(1-\alpha_{1k}^{(l)})||\beta_k^{(l)}||_2^2 \right],  \label{beta_up_m}
\\
    {\hat{\gamma}}^{(t+1)} &amp; =  \operatorname{argmax}_{\gamma \in
\mathbb{R}^q} \sum_{l=1}^L \sum_{k=1}^K \sum_{i=1}^n
r_{ik}^{(l),(t)}\operatorname{log} \pi_{ik}({w_i},{\gamma})
-  \lambda_2\left[ \alpha_2 |{\gamma}| + \frac{1}{2}(1 - \alpha_2)
||\gamma||_2^2 \right]. \label{gamma_up_m}
\end{align}\]</span></p>
<p>Equations <span class="math inline">\(\hat{\beta}_k^{(l),(t+1)}\)</span> and <span class="math inline">\({\hat{\gamma}}^{(t+1)}\)</span> also can be solved
by a proximal Newton method.</p>
</div>
</div>
<div id="parameter-tunning" class="section level1">
<h1 class="hasAnchor">
<a href="#parameter-tunning" class="anchor"></a>Parameter Tunning</h1>
<p>In this section, we will give details of how NEMoE select tuning
parameters. These procedures has been wrapped up in function
<code>cvNEMoE</code> in our package (The usage of function can be found
in <code>Get started</code> -&gt;
<code>Evaluation and cross validation of NEMoE</code>).</p>
<div id="generate-candidates-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#generate-candidates-parameters" class="anchor"></a>Generate candidates parameters</h2>
<p>In NEMoE, two parameters <span class="math inline">\(\lambda_1\)</span> (similar to <span class="math inline">\(\lambda_{1k}\)</span>) and <span class="math inline">\(\lambda_2\)</span> (similar to <span class="math inline">\(\lambda_{2k}\)</span>) need to optimize (by
default we set parameters corresponding to <span class="math inline">\(\alpha\)</span> be 0.5), we generate a series of
candidates using following procedure, which is the same strategy used in
package <code>glmnet</code><span class="citation">(Friedman, Hastie, and
Tibshirani 2010)</span>.</p>
<p><strong>Step 1:</strong> Set a sequence for <span class="math inline">\(\lambda_2\)</span>. In our package, we use <span class="math inline">\(\lambda_2=(0.005, 0.014, 0.016, 0.02, 0.03,
0.05)\)</span> as default.</p>
<p><strong>Step 2:</strong> Set maximum and minimum of <span class="math inline">\(\lambda_1\)</span>, denote as <span class="math inline">\(\lambda_{1max}\)</span> and <span class="math inline">\(\lambda_{1min}\)</span>. In NEMoE package, <span class="math inline">\(\lambda_{1max}\)</span> is determined using
backtracking (See Backtracking search section) and <span class="math inline">\(\lambda_{1min} = 0.01\lambda_{1max}\)</span>.</p>
<p><strong>Step 3:</strong> Generate sequence of <span class="math inline">\(\lambda_1\)</span> with <span class="math inline">\(g_1\)</span> candidates, i.e. <span class="math inline">\(\lambda_1=(\lambda_1^{(1)}, \ldots,
\lambda_1^{(g_1)})\)</span>, by</p>
<p><span class="math display">\[\begin{equation}
\lambda_1^{(i)} = \lambda_{1min}exp\{ (i-1) C\}, \quad i=1,\ldots,g_1
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C=\frac{log(\lambda_{1max}) -
log(\lambda_{1min}))}{g_1 - 1}\)</span></p>
<div id="backtracking-search" class="section level3">
<h3 class="hasAnchor">
<a href="#backtracking-search" class="anchor"></a>Backtracking search</h3>
<p>For generating <span class="math inline">\(\lambda_{1max}\)</span>,
we use the following backtracking search strategy:</p>
<p><strong>Step 1:</strong> Generate a large enough <span class="math inline">\(\lambda_{1max}\)</span>. In our package, we use
<span class="math inline">\(\lambda_{1max} =
max(\frac{X_i^Ty}{n})\)</span></p>
<p><strong>Step 2:</strong> Fitting NEMoE with few iterations (in NEMoE,
we use 5 iterations).</p>
<p><strong>Step 3:</strong> If the fitted coefficients have at least one
non-zero coefficients, stop. Or set <span class="math inline">\(\lambda_{1max} = 0.8\lambda_{1max}\)</span> and do
<strong>Step 2</strong>.</p>
</div>
</div>
<div id="calculate-evaluation-criterion" class="section level2">
<h2 class="hasAnchor">
<a href="#calculate-evaluation-criterion" class="anchor"></a>Calculate evaluation criterion</h2>
<p>After generating candidate parameters, we fit corresponding NEMoE
model with each parameter based on previous section. To select
appropriate parameters, in our NEMoE package, we provide variety of
evaluation criterion listed below. All of these have been available in
function <code>calcCriterion</code> in our package.</p>
<table class="table">
<thead><tr class="header">
<th align="right">Criterion</th>
<th align="center">Calculation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">AIC</td>
<td align="center"><span class="math inline">\(-2LL(\hat{\beta},\hat{\gamma}) +
2||\hat{\gamma}||_0+2\sum_{k=1}^K||\hat{\beta_k}||_0\)</span></td>
</tr>
<tr class="even">
<td align="right">BIC</td>
<td align="center"><span class="math inline">\(-2LL(\hat{\beta},\hat{\gamma}) +
||\hat{\gamma}||_0log(n)+\sum_{k=1}^Klog(\sum_{i=1}^n \pi_{ik})
||\hat{\beta_k}||_0\)</span></td>
</tr>
<tr class="odd">
<td align="right">eBIC</td>
<td align="center"><span class="math inline">\(-2LL(\hat{\beta},\hat{\gamma}) +
||\hat{\gamma}||_0[log(n)+log(p)]+\sum_{k=1}^Klog(\sum_{i=1}^n \pi_{ik})
||\hat{\beta_k}||_0 + \sum_{k=1}^K
||\hat{\beta_k}||_0log(p)\)</span></td>
</tr>
<tr class="even">
<td align="right">ICL</td>
<td align="center"><span class="math inline">\(-2LL(\hat{\beta},\hat{\gamma}) +
||\hat{\gamma}||_0log(n)+\sum_{k=1}^Klog(\sum_{i=1}^n \pi_{ik})
||\hat{\beta_k}||_0+\sum_{k=1}^K\sum_{i=1}^nr_{ik}log(r_{ik})\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\gamma}\)</span> are the estimation of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span>. <span class="math inline">\(||\cdot||_0\)</span> is the vector zero norm,
i.e. the number of non-zero values in the vector. AIC, BIC are widely
used in statistics when select tuning parameters<span class="citation">Khalili and Chen (2007)</span>. eBIC (extended BIC) is
designed for high dimensional settings<span class="citation">(Chen and
Chen 2008)</span>. ICL stands for integrated classification criterion is
widely used in mixture model<span class="citation">(Biernacki, Celeux,
and Govaert 2000)</span>.</p>
</div>
</div>
<div id="reference" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#reference" class="anchor"></a>Reference</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-biernacki2000assessing" class="csl-entry">
Biernacki, Christophe, Gilles Celeux, and Gérard Govaert. 2000.
<span>“Assessing a Mixture Model for Clustering with the Integrated
Completed Likelihood.”</span> <em>IEEE Transactions on Pattern Analysis
and Machine Intelligence</em> 22 (7): 719–25.
</div>
<div id="ref-chen2008extended" class="csl-entry">
Chen, Jiahua, and Zehua Chen. 2008. <span>“Extended Bayesian Information
Criteria for Model Selection with Large Model Spaces.”</span>
<em>Biometrika</em> 95 (3): 759–71.
</div>
<div id="ref-friedman2010regularization" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2010.
<span>“Regularization Paths for Generalized Linear Models via Coordinate
Descent.”</span> <em>Journal of Statistical Software</em> 33 (1): 1.
</div>
<div id="ref-hunter2004tutorial" class="csl-entry">
Hunter, David R, and Kenneth Lange. 2004. <span>“A Tutorial on MM
Algorithms.”</span> <em>The American Statistician</em> 58 (1): 30–37.
</div>
<div id="ref-khalili2007variable" class="csl-entry">
Khalili, Abbas, and Jiahua Chen. 2007. <span>“Variable Selection in
Finite Mixture of Regression Models.”</span> <em>Journal of the American
Statistical Association</em> 102 (479): 1025–38.
</div>
<div id="ref-mclachlan2019finite" class="csl-entry">
McLachlan, Geoffrey J, Sharon X Lee, and Suren I Rathnayake. 2019.
<span>“Finite Mixture Models.”</span> <em>Annual Review of Statistics
and Its Application</em> 6: 355–78.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Xiangnan Xu.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
